import sys
import os

# Add the src directory to the Python path to enable absolute imports
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), 'src')))

from llm_integration.google_gemini_connector import GoogleGeminiConnector
from llm_integration.openai_chatgpt_connector import OpenAIChatGPTConnector # Import OpenAIChatGPTConnector
from agent.basic_agent import BasicAgent
from agent.communication import AgentCommunicationChannel
from memory.sqlite_memory import SQLiteMemory # Import SQLiteMemory
from plugins.plugin_manager import PluginManager
from task_management.task import Task
from orchestrator.simple_orchestrator import SimpleOrchestrator
from planning.planner import BasicPlanner # Import BasicPlanner

def main():
    """
    Main function to set up and run the LLMSuperAgent CLI.
    """
    print("Initializing LLMSuperAgent...")

    # Initialize core components
    communication_channel = AgentCommunicationChannel()
    memory = SQLiteMemory() # Use SQLiteMemory instead of in-memory Memory
    plugin_manager = PluginManager()

    # Create LLM connectors (using GoogleGeminiConnector)
    # Note: Ensure GOOGLE_API_KEY environment variable is set for this to work
    # Using 'models/gemini-2.5-flash-preview-04-17' as it supports generateContent
    # Create LLM connectors
    # Note: Ensure GOOGLE_API_KEY and OPENAI_API_KEY environment variables are set
    gemini_llm = GoogleGeminiConnector("models/gemini-2.5-flash-preview-04-17") # Use a real Gemini model name
    openai_llm = OpenAIChatGPTConnector("gpt-4") # Use a real OpenAI model name

    # Create basic agents, passing the necessary components and the Gemini connector
    # Create basic agents, passing the necessary components and the LLM connectors
    agent_gemini = BasicAgent("GeminiAgent", gemini_llm, communication_channel, memory, plugin_manager)
    agent_openai = BasicAgent("OpenAIAgent", openai_llm, communication_channel, memory, plugin_manager)

    # Create a planner instance
    planner = BasicPlanner() # Use the BasicPlanner for now

    # Create the orchestrator with the agents and the planner
    orchestrator = SimpleOrchestrator(agents=[agent_gemini, agent_openai], planner=planner)

    print("LLMSuperAgent initialized. Type 'exit' to quit.")

    # Basic CLI loop
    while True:
        try:
            user_input = input("Enter a high-level goal: ")
            if user_input.lower() == 'exit':
                break

            if not user_input.strip():
                print("Please enter a valid task description.")
                continue

            # Process the high-level goal using the orchestrator's planning capability
            orchestrator.process_goal(user_input.strip())

            # Run the orchestrator to process the tasks generated by the planner
            orchestrator.run() # In a real system, this might run in a separate thread or process

            # Display the result of the task (assuming sequential processing for MVP)
            # In a more complex system, you'd retrieve the task by ID and check its status/output
            print("\nTask Processing Complete.")
            # For this simple example, we'll just show the last processed task's output if available
            # A better approach would be to retrieve the task from a task registry
            # For now, we'll rely on the orchestrator's logging to show processing.

        except Exception as e:
            print(f"An error occurred: {e}")

    print("Exiting LLMSuperAgent CLI.")

if __name__ == "__main__":
    main()